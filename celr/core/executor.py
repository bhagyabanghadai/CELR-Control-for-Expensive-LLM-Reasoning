"""
Task Executor â€” the main execution loop for CELR.

This is the heart of the system. It:
  1. Gets runnable steps from the plan (respecting dependency order)
  2. Routes each step to the right model via EscalationManager
  3. Dispatches to LLM or ToolRegistry based on StepType
  4. Verifies results with Verifier
  5. On failure: runs SelfReflection â†’ retries with better approach
  6. Tracks cost for every call

Key improvements (Overhaul Phase O-3):
  - Verifier, Reflection, and ToolRegistry are fully wired
  - Real LLM dispatch replaces mock execution
  - Verify-then-reflect retry loop with max_retries
  - Uses custom exceptions (BudgetExhaustedError)
  - Returns structured result with cost summary
"""

import logging
import time
from typing import Optional

from celr.core.cost_tracker import CostTracker
from celr.core.escalation import EscalationManager
from celr.core.exceptions import BudgetExhaustedError, ToolExecutionError
from celr.core.llm import BaseLLMProvider, LLMUsage
from celr.core.planner import Planner
from celr.core.logger import TrajectoryLogger
from celr.core.prompts import SEMANTIC_VERIFICATION_PROMPT
from celr.core.reflection import SelfReflection
from celr.core.tools import ToolRegistry
from celr.core.types import Plan, Step, StepType, TaskContext, TaskStatus
from celr.core.verifier import Verifier
from celr.cortex import StateExtractor, MetaPolicy
from celr.cortex.policy import CortexAction  # Explicit import
from celr.cortex.council import HiveMindCouncil, Verdict  # Phase 9: Hive-Mind

logger = logging.getLogger(__name__)


class TaskExecutor:
    """
    Main execution engine.
    
    Orchestrates: Plan â†’ Route â†’ Execute â†’ Verify â†’ (Reflect â†’ Retry) â†’ Log
    """

    def __init__(
        self,
        context: TaskContext,
        planner: Planner,
        cost_tracker: CostTracker,
        escalation_manager: EscalationManager,
        tool_registry: ToolRegistry,
        verifier: Verifier,
        reflection: SelfReflection,
        trajectory_logger: Optional[TrajectoryLogger] = None,
    ):
        self.context = context
        self.planner = planner
        self.tracker = cost_tracker
        self.escalation = escalation_manager
        self.tools = tool_registry
        self.verifier = verifier
        self.reflection = reflection
        self.trajectory_logger = trajectory_logger
        
        # Phase 5: Stateful Runtime
        from celr.core.runtime import PersistentRuntime
        self.runtime = PersistentRuntime()
        
        # Phase 8: Adaptive Cortex
        self.state_extractor = StateExtractor()
        self.policy = MetaPolicy()
        self.current_plan: Optional[Plan] = None

        # Phase 9: Hive-Mind Council (lazy-init, only used on ESCALATE)
        self._council: Optional[HiveMindCouncil] = None

    def run(self, plan: Plan) -> str:
        """
        Main execution loop.
        
        1. Get runnable steps (dependencies met).
        2. Execute each step (route â†’ dispatch â†’ verify â†’ reflect).
        3. Repeat until done, failed, or stuck.
        
        Returns:
            "SUCCESS", "FAILED", or "STUCK"
        """
        self.context.log("Starting execution loop...")
        logger.info(f"Executing plan with {len(plan.items)} steps")
        self.current_plan = plan # For Cortex state extraction

        while True:
            runnable_steps = self.planner.get_ready_steps(plan)

            # Check for completion
            if not runnable_steps:
                if all(s.status in [TaskStatus.COMPLETED, TaskStatus.SKIPPED] for s in plan.items):
                    self.context.log("All steps completed successfully.")
                    logger.info("Plan completed successfully")
                    return "SUCCESS"

                failed_steps = [s for s in plan.items if s.status == TaskStatus.FAILED]
                if failed_steps:
                    self.context.log(f"Plan failed. {len(failed_steps)} steps failed.")
                    logger.error(f"Plan failed: {len(failed_steps)} steps failed")
                    return "FAILED"

                # Pending steps but none runnable = cycle or bug
                self.context.log("Stuck! No runnable steps but plan is not complete.")
                logger.error("Execution stuck: no runnable steps")
                if self.trajectory_logger:
                    self.trajectory_logger.update(self.context, plan, "STUCK")
                return "STUCK"

            if self.trajectory_logger:
                self.trajectory_logger.update(self.context, plan, "RUNNING")

            for step in runnable_steps:
                try:
                    self._execute_with_retries(step)
                except BudgetExhaustedError as e:
                    self.context.log(f"Budget exhausted: {e}")
                    logger.error(f"Budget exhausted: {e}")
                    step.status = TaskStatus.FAILED
                    return "FAILED"

            # Throttle to avoid tight loops
            time.sleep(0.1)

    def _execute_with_retries(self, step: Step) -> None:
        """
        Execute a step with the Verify â†’ Reflect â†’ Retry loop.
        
        For each attempt:
          1. Dispatch (LLM call or tool execution)
          2. Verify the result
          3. If verification fails: analyze failure â†’ update step â†’ retry
        """
        self.context.log(f"Executing step: {step.description} ({step.id})")
        step.status = TaskStatus.RUNNING

        for attempt in range(step.max_retries + 1):
            step.retry_count = attempt

            if attempt > 0:
                self.context.log(f"Retry {attempt}/{step.max_retries} for step {step.id}")
                logger.info(f"Retry {attempt}/{step.max_retries} for step {step.id}")
            
            # Live update start of step attempt
            if self.trajectory_logger and self.current_plan:
                self.trajectory_logger.update(self.context, self.current_plan, "RUNNING")

            try:
                # --- Phase 8: Adaptive Cortex Decision ---
                # 1. Extract State
                current_plan_idx = 0 
                if self.current_plan:
                    try:
                        current_plan_idx = [s.id for s in self.current_plan.items].index(step.id)
                    except ValueError:
                        pass
                
                state = self.state_extractor.extract(
                    self.context, 
                    self.current_plan, 
                    current_step_idx=current_plan_idx, 
                    retry_count=attempt
                )
                
                # Save state for logging/dashboard
                step._last_state_vector = state.tolist()

                # 2. Get Action from Policy
                action = self.policy.get_action(state)
                logger.info(f"Cortex Action for step {step.id}: {action}")

                force_expensive = False
                
                if action == CortexAction.ABORT:
                    self.context.log(f"ðŸ›‘ Cortex aborted step {step.id} (Safety/Cost risk)")
                    logger.warning("Cortex triggered ABORT")
                    step.status = TaskStatus.FAILED
                    break # Stop retries
                
                elif action == CortexAction.ESCALATE:
                    # â”€â”€ Phase 9: Hive-Mind Council deliberation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # Before committing to the expensive model, ask the
                    # Council of Experts to vote on whether escalation is
                    # truly needed.  All three critics fire in PARALLEL.
                    if self._council is None:
                        self._council = HiveMindCouncil()
                    proposal = (
                        f"Should we escalate step '{step.description}' "
                        f"(attempt {attempt + 1}) to a more expensive model? "
                        f"Budget remaining: ${self.tracker.remaining:.4f}"
                    )
                    debate = self._council.deliberate(proposal)
                    self.context.log(f"ðŸ§  {debate.summary}")
                    logger.info(f"Council verdict: {debate.final_verdict.value}")

                    # Store debate in context for dashboard
                    self.context.council_debates.append(debate.model_dump())

                    if debate.final_verdict == Verdict.APPROVE:
                        self.context.log("âš¡ Council approved ESCALATION â†’ using stronger model")
                        force_expensive = True
                    else:
                        self.context.log("ðŸš« Council REJECTED escalation â†’ staying with cheap model")
                        force_expensive = False
                    
                    # Update dashboard with debate results
                    if self.trajectory_logger:
                        self.trajectory_logger.update(self.context, self.current_plan, "RUNNING")
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                elif action == CortexAction.STOP:
                    # Rare case: Early success prediction?
                    # For now, treat as proceed normally.
                    pass

                # 3. Budget check (Legacy safety net)
                estimated_cost = 0.01
                if not self.tracker.can_afford(estimated_cost):
                    raise BudgetExhaustedError(
                        budget_limit=self.context.budget_limit_usd,
                        current_spend=self.context.current_spread_usd,
                        step_description=step.description,
                    )

                # 4. Dispatch (route + execute)
                # Pass force_expensive flag if Cortex requested escalation
                if step.step_type == StepType.EXECUTION:
                     # Tools don't use LLM provider directly in dispatch_tool yet, 
                     # but _get_code_from_llm does.
                     result = self._dispatch_tool(step, force_expensive)
                else:
                     result = self._dispatch_llm(step, force_expensive)
                
                step.output = result

                # 3. Verify
                is_valid = self.verifier.verify(step, self.context)

                if is_valid:
                    step.status = TaskStatus.COMPLETED
                    self.context.log(
                        f"Step {step.id} completed (attempt {attempt + 1}). "
                        f"output_len={len(result)}"
                    )
                    logger.info(f"Step {step.id} verified OK on attempt {attempt + 1}")
                    return
                else:
                    # 4. Verification failed â†’ Reflect
                    self.context.log(
                        f"Step {step.id} verification failed: {step.verification_notes}"
                    )

                    if self.reflection.should_retry(step, attempt):
                        # Get reflection analysis and enhance the step description
                        fix = self.reflection.analyze_failure(step, self.context)
                        step.description += f"\n[Retry {attempt + 1} fix]: {fix}"
                        logger.info(f"Reflection suggested fix for step {step.id}")
                    else:
                        logger.warning(f"Step {step.id} failed verification, no more retries")
                        break

            except BudgetExhaustedError:
                raise  # Propagate budget errors up
            except ToolExecutionError as e:
                self.context.log(f"Tool error on step {step.id}: {e}")
                logger.error(f"Tool execution failed: {e}")
                step.output = str(e)
                if not self.reflection.should_retry(step, attempt):
                    break
            except Exception as e:
                self.context.log(f"Step {step.id} failed: {e}")
                logger.exception(f"Unexpected error on step {step.id}")
                step.output = str(e)
                if not self.reflection.should_retry(step, attempt):
                    break

        # All retries exhausted
        step.status = TaskStatus.FAILED
        self.context.log(f"Step {step.id} FAILED after {step.retry_count + 1} attempts.")
        logger.error(f"Step {step.id} failed after {step.retry_count + 1} attempts")

    def _dispatch(self, step: Step) -> str:
        """
        Route and execute a step based on its type.
        
        - EXECUTION â†’ ToolRegistry
        - REASONING â†’ LLM call via EscalationManager
        - VERIFICATION â†’ LLM call (verifier handles this separately)
        """
        if step.step_type == StepType.EXECUTION:
            return self._dispatch_tool(step)
        else:
            return self._dispatch_llm(step)

    def _dispatch_tool(self, step: Step, force_expensive: bool = False) -> str:
        """Execute a tool-based step."""
        # Extract tool name and code from description
        # Convention: step description starts with "TOOL:tool_name:" for tool steps
        if step.description.startswith("TOOL:"):
            parts = step.description.split(":", 2)
            tool_name = parts[1] if len(parts) > 1 else "python_repl"
            code = parts[2] if len(parts) > 2 else step.description
        else:
            # Default: ask LLM for code, then execute
            tool_name = "python_repl"
            code = self._get_code_from_llm(step, force_expensive=force_expensive)

        # Execution-Guided Repair Loop (Phase 2)
        max_code_retries = 3
        last_error = None
        
        for attempt in range(max_code_retries):
            # If retrying, ask for fix
            code = self._get_code_from_llm(step, force_expensive=force_expensive, error_context=last_error)
            
            # [Phase 3] Semantic Verification (Recursive Self-Correction)
            # Before executing, check if the code matches the prompt's data exactly.
            verification_result = self._verify_step_semantics(step, code, force_expensive=force_expensive)
            if verification_result.startswith("MISMATCH"):
                last_error = f"Semantic Verification Failed: {verification_result}\nPlease fix the code to match the user request exactly."
                self.context.log(f"Semantic Check Failed: {verification_result}")
                continue # Skip execution, retry with fix
            
            self.context.log(f"Dispatching to tool: {tool_name} (Attempt {attempt+1}/{max_code_retries})")
            
            # Phase 5: Stateful Execution
            # Execute code in persistent runtime
            result, success = self.runtime.execute(code)
            
            # Check for execution errors
            if not success:
                last_error = result
                self.context.log(f"Code execution failed on attempt {attempt+1}. Retrying...")
                continue
            
            # Success!
            # OPTIMIZATION for Benchmarks (HumanEval):
            # Always output the code so the grader can check it,
            # even if there is execution output (e.g. test prints).
            result += f"\n\n[Captured Code]\n{code}"
            return result
            
        # If all retries failed, return the last error result (with code)
        return f"Execution failed after {max_code_retries} attempts.\nLast Error:\n{last_error}\n\n[Captured Code]\n{code}"

    def _dispatch_llm(self, step: Step, force_expensive: bool = False) -> str:
        """Execute a reasoning/verification step via LLM."""
        # Get the right provider based on difficulty + budget
        provider = self.escalation.get_provider(step, force_expensive=force_expensive)

        prompt = (
            f"Task: {step.description}\n\n"
            f"Context (recent history):\n{self.context.execution_history[-5:]}\n\n"
            f"System Instructions:\n"
            f"1. Provide a clear, actionable response.\n"
            f"2. Do NOT hallucinate data. Use ONLY values provided in the task.\n"
            f"3. For math problems, calculate explicitly. Do NOT use `input()`.\n"
            f"4. If writing code, ensure it is self-contained and prints the result.\n"
        )

        text, usage = provider.generate(prompt)

        # Track cost
        cost = provider.calculate_cost(usage)
        self.tracker.add_cost(cost)
        step.cost_usd = cost

        self.context.log(
            f"LLM call: {step.assigned_agent}, "
            f"tokens={usage.total_tokens}, cost=${cost:.6f}"
        )

        return text

    def _get_code_from_llm(self, step: Step, force_expensive: bool = False, error_context: Optional[str] = None) -> str:
        """Ask LLM to generate executable code for a tool step."""
        provider = self.escalation.get_provider(step, force_expensive=force_expensive)
        
        # Phase 5: Stateful Prompting
        # Get current variable state from runtime
        runtime_context = self.runtime.get_context_snapshot()
        
        # Include recent history so the model knows results from previous steps
        history_text = "\n".join(self.context.execution_history[-5:])
        
        # Phase 6: Multi-Agent "Team of Experts" Routing
        from celr.core.agents import AgentFactory
        assigned_role = self.context.shared_state.get("assigned_agent", "CODER")
        specialist = AgentFactory.get_agent(assigned_role)
        
        # Base Prompt updated for Simulated REPL with Ground Truth
        # We append the Specialist System Prompt to the Base Prompt
        
        base_prompt = (
            f"Original User Task:\n{self.context.original_request}\n\n"
            f"Current Step Task:\n{step.description}\n\n"
            f"Active Variables (DO NOT REDEFINE THESE):\n{runtime_context}\n\n"
            f"Recent Output History:\n{history_text}\n\n"
            f"Constraints:\n"
            f"1. Return ONLY the code, no explanation.\n"
            f"2. Use existing variables from 'Active Variables' directly.\n"
            f"3. Do NOT use `input()`. Use variables for values.\n"
            f"4. Print the final result explicitly.\n"
            f"5. Extract numbers/data from 'Original User Task' if not in 'Active Variables'.\n"
        )
        
        # Inject the Specialist System Prompt
        full_prompt = (
            f"{specialist.system_prompt}\n\n"
            f"{base_prompt}"
        )
        
        if error_context:
            prompt = (
                f"{full_prompt}\n"
                f"PREVIOUS CODE FAILED WITH ERROR:\n"
                f"{error_context}\n"
                f"Fix the code and output ONLY the corrected code."
            )
        else:
            prompt = full_prompt

        code, usage = provider.generate(prompt)
        cost = provider.calculate_cost(usage)
        self.tracker.add_cost(cost)
        
        # Strip markdown formatting if present
        if "```" in code:
            code = code.replace("```python", "").replace("```py", "").replace("```", "").strip()
            
        return code

    def _verify_step_semantics(self, step: Step, code: str, force_expensive: bool = False) -> str:
        """
        Phase 3: Recursive Self-Correction (CRITIC AGENT).
        Asks the CRITIC Agent to verify if the generated code matches the prompt's specific numbers/data.
        Returns "CORRECT" or "MISMATCH: explanation".
        """
        from celr.core.agents import AgentFactory
        
        provider = self.escalation.get_provider(step, force_expensive=force_expensive)
        critic = AgentFactory.get_agent("CRITIC")
        
        # Combine Critic System Prompt with the specific verification task
        prompt = (
            f"{critic.system_prompt}\n\n"
            f"User Request: {step.description}\n"
            f"Generated Logic/Code:\n{code}\n\n"
            f"Analyze strict compliance."
        )
        
        response, usage = provider.generate(prompt)
        # Track verifying cost
        cost = provider.calculate_cost(usage)
        self.tracker.add_cost(cost)
        
        return response.strip()
